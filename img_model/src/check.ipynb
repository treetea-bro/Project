{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os.path as osp\n",
        "\n",
        "import cv2\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "import torch.hub\n",
        "import os\n",
        "import model\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "from visualize.grad_cam import BackPropagation, GradCAM, GuidedBackPropagation\n",
        "\n",
        "faceCascade = cv2.CascadeClassifier('./visualize/haarcascade_frontalface_default.xml')\n",
        "shape = (48, 48)\n",
        "classes = [\n",
        "    'Angry',\n",
        "    'Disgust',\n",
        "    'Fear',\n",
        "    'Happy',\n",
        "    'Sad',\n",
        "    'Surprised',\n",
        "    'Neutral'\n",
        "]\n",
        "\n",
        "\n",
        "def preprocess(image_path):\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    image = cv2.imread(image_path)\n",
        "    faces = faceCascade.detectMultiScale(\n",
        "        image,\n",
        "        scaleFactor=1.1,\n",
        "        minNeighbors=5,\n",
        "        minSize=(1, 1),\n",
        "        flags=cv2.CASCADE_SCALE_IMAGE\n",
        "    )\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        print('no face found')\n",
        "        face = cv2.resize(image, shape)\n",
        "    else:\n",
        "        (x, y, w, h) = faces[0]\n",
        "        face = image[y:y + h, x:x + w]\n",
        "        face = cv2.resize(face, shape)\n",
        "\n",
        "    img = Image.fromarray(face).convert('L')\n",
        "    inputs = transform_test(img)\n",
        "    return inputs, face\n",
        "\n",
        "\n",
        "def get_gradient_image(gradient):\n",
        "    gradient = gradient.cpu().numpy().transpose(1, 2, 0)\n",
        "    gradient -= gradient.min()\n",
        "    gradient /= gradient.max()\n",
        "    gradient *= 255.0\n",
        "    return np.uint8(gradient)\n",
        "\n",
        "\n",
        "def get_gradcam_image(gcam, raw_image, paper_cmap=False):\n",
        "    gcam = gcam.cpu().numpy()\n",
        "    cmap = cm.jet_r(gcam)[..., :3] * 255.0\n",
        "    if paper_cmap:\n",
        "        alpha = gcam[..., None]\n",
        "        gcam = alpha * cmap + (1 - alpha) * raw_image\n",
        "    else:\n",
        "        gcam = (cmap.astype(np.float64) + raw_image.astype(np.float64)) / 2\n",
        "    return np.uint8(gcam)\n",
        "\n",
        "\n",
        "def guided_backprop(images, model_name):\n",
        "    for i, image in enumerate(images):\n",
        "        target, raw_image = preprocess(image['path'])\n",
        "        image['image'] = target\n",
        "        image['raw_image'] = raw_image\n",
        "\n",
        "    net = model.Model(num_classes=len(classes))\n",
        "    checkpoint = torch.load(os.path.join('../trained', model_name), map_location=torch.device('cpu'))\n",
        "    net.load_state_dict(checkpoint['net'])\n",
        "    net.eval()\n",
        "    summary(net, (1, shape[0], shape[1]))\n",
        "\n",
        "    result_images = []\n",
        "    for index, image in enumerate(images):\n",
        "        img = torch.stack([image['image']])\n",
        "        bp = BackPropagation(model=net)\n",
        "        probs, ids = bp.forward(img)\n",
        "        gcam = GradCAM(model=net)\n",
        "        _ = gcam.forward(img)\n",
        "\n",
        "        gbp = GuidedBackPropagation(model=net)\n",
        "        _ = gbp.forward(img)\n",
        "\n",
        "        # Guided Backpropagation\n",
        "        actual_emotion = ids[:, 0]\n",
        "        gbp.backward(ids=actual_emotion.reshape(1, 1))\n",
        "        gradients = gbp.generate()\n",
        "\n",
        "        # Grad-CAM\n",
        "        gcam.backward(ids=actual_emotion.reshape(1, 1))\n",
        "        regions = gcam.generate(target_layer='last_conv')\n",
        "\n",
        "        # Get Images\n",
        "        label_image = np.zeros((shape[0], 65, 3), np.uint8)\n",
        "        cv2.putText(label_image, classes[actual_emotion.data], (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255),\n",
        "                    1, cv2.LINE_AA)\n",
        "\n",
        "        prob_image = np.zeros((shape[0], 60, 3), np.uint8)\n",
        "        cv2.putText(prob_image, '%.1f%%' % (probs.data[:, 0] * 100), (5, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                    (255, 255, 255), 1, cv2.LINE_AA)\n",
        "\n",
        "        guided_bpg_image = get_gradient_image(gradients[0])\n",
        "        guided_bpg_image = cv2.merge((guided_bpg_image, guided_bpg_image, guided_bpg_image))\n",
        "\n",
        "        grad_cam_image = get_gradcam_image(gcam=regions[0, 0], raw_image=image['raw_image'])\n",
        "\n",
        "        guided_gradcam_image = get_gradient_image(torch.mul(regions, gradients)[0])\n",
        "        guided_gradcam_image = cv2.merge((guided_gradcam_image, guided_gradcam_image, guided_gradcam_image))\n",
        "\n",
        "        img = cv2.hconcat(\n",
        "            [image['raw_image'], label_image, prob_image, guided_bpg_image, grad_cam_image, guided_gradcam_image])\n",
        "        result_images.append(img)\n",
        "        print(image['path'], classes[actual_emotion.data], probs.data[:, 0] * 100)\n",
        "\n",
        "    cv2.imwrite('../test/guided_gradcam.jpg', cv2.resize(cv2.vconcat(result_images), None, fx=2, fy=2))\n",
        "\n",
        "\n",
        "def main():\n",
        "    guided_backprop(\n",
        "        images=[\n",
        "            {'path': '../test/a_resized_1.png'},\n",
        "            {'path': '../test/a_resized_2.png'},\n",
        "            {'path': '../test/a_resized_3.png'},\n",
        "            {'path': '../test/a_resized_4.png'},\n",
        "            {'path': '../test/a_resized_5.png'},\n",
        "            {'path': '../test/b_resized_1.png'},\n",
        "            {'path': '../test/b_resized_2.png'},\n",
        "            {'path': '../test/b_resized_3.png'},\n",
        "            {'path': '../test/b_resized_4.png'},\n",
        "            {'path': '../test/b_resized_5.png'},\n",
        "            {'path': '../test/c_resized_1.png'},\n",
        "            {'path': '../test/c_resized_2.png'},\n",
        "            {'path': '../test/c_resized_3.png'},\n",
        "            {'path': '../test/c_resized_4.png'},\n",
        "            {'path': '../test/c_resized_5.png'},\n",
        "        ],\n",
        "        model_name='private_model_233_66.t7'\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}